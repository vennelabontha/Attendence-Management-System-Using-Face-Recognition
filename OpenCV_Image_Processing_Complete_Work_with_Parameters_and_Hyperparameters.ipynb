{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vennelabontha/Attendence-Management-System-Using-Face-Recognition/blob/main/OpenCV_Image_Processing_Complete_Work_with_Parameters_and_Hyperparameters.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97ee444e",
      "metadata": {
        "id": "97ee444e"
      },
      "source": [
        "\n",
        "# OpenCV Image Processing Demonstrations with Detailed Parameter Explanations and Hyperparameters\n",
        "\n",
        "Welcome to this notebook where we will learn about various image processing techniques using OpenCV. Each section contains explanations about the techniques, the required code, detailed information about each parameter, hyperparameters that can be used for experimentation, and a Before vs After comparison to help you understand the changes made to the images.\n",
        "\n",
        "## Topics Covered:\n",
        "1. **Edge Detection** - Finding the edges in an image.\n",
        "2. **Image Sharpening** - Making an image look clearer by highlighting edges.\n",
        "3. **Blur Image** - Softening an image to reduce noise or detail.\n",
        "4. **Image Resize** - Making an image bigger or smaller.\n",
        "5. **Image Rotation** - Changing the orientation of an image.\n",
        "6. **Image Augmentation** - Modifying an image by flipping it.\n",
        "7. **Image Cropping** - Cutting out part of an image.\n",
        "8. **Convert Image to Black and White and Negative** - Changing color representation.\n",
        "9. **Face Detection** - Finding faces in an image.\n",
        "10. **Identifying Facial Features** - Identifying eyes in a face.\n",
        "\n",
        "Let's get started!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "091df8ed",
      "metadata": {
        "id": "091df8ed",
        "outputId": "241cf3d8-c5d3-4837-d888-43368623dd1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "OpenCV(4.10.0) /io/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0dbfdcfd9749>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mimage_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Couple.jpg'\u001b[0m \u001b[0;31m# Change to your image path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mimage_rgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Widget to display Before vs After images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.10.0) /io/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "# Load Image\n",
        "image_path = 'Couple.jpg' # Change to your image path\n",
        "image = cv2.imread(image_path)\n",
        "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Widget to display Before vs After images\n",
        "def display_before_after(before, after, title_before='Before', title_after='After'):\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
        "    axs[0].imshow(before)\n",
        "    axs[0].set_title(title_before)\n",
        "    axs[0].axis('off')\n",
        "    axs[1].imshow(after)\n",
        "    axs[1].set_title(title_after)\n",
        "    axs[1].axis('off')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80e3995e",
      "metadata": {
        "id": "80e3995e"
      },
      "source": [
        "\n",
        "## 1. Edge Detection\n",
        "\n",
        "Edge detection is used to find the boundaries within images by detecting rapid changes in pixel intensity. We use the **Canny Edge Detection** method, which requires two threshold values to determine the edges.\n",
        "\n",
        "- **Syntax**: `cv2.Canny(image, threshold1, threshold2)`\n",
        "  - **`threshold1`**: The lower threshold for the intensity gradient. Recommended values are between `50-150`.\n",
        "  - **`threshold2`**: The upper threshold for the intensity gradient. This value should be roughly double `threshold1`.\n",
        "  - **Hyperparameters to Experiment With**:\n",
        "    - `threshold1 = 50`, `threshold2 = 100`: Lower threshold values to include more edges.\n",
        "    - `threshold1 = 100`, `threshold2 = 200`: Balanced approach for sharper edges.\n",
        "    - `threshold1 = 150`, `threshold2 = 300`: Higher thresholds to capture only the most prominent edges.\n",
        "  - **Why These Values**: Experimenting with threshold values helps to understand the sensitivity of edge detection. Lower values detect more edges but may also increase noise.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1570857f",
      "metadata": {
        "id": "1570857f",
        "outputId": "deccd00d-16ad-425c-9edf-a6a532d3d8e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'display_before_after' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-55a71254e312>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Edge Detection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0medges\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCanny\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdisplay_before_after\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_rgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medges\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Original Image'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Edges Detected'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'display_before_after' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "# Edge Detection\n",
        "edges = cv2.Canny(image, 100, 200)\n",
        "display_before_after(image_rgb, edges, 'Original Image', 'Edges Detected')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c611a0ca",
      "metadata": {
        "id": "c611a0ca"
      },
      "source": [
        "\n",
        "## 2. Image Sharpening\n",
        "\n",
        "Image sharpening is used to enhance the edges and make the image appear more defined. We use a **kernel** (a small matrix) to process the image.\n",
        "\n",
        "- **Kernel Used**:\n",
        "  ```\n",
        "  -1 -1 -1\n",
        "  -1  9 -1\n",
        "  -1 -1 -1\n",
        "  ```\n",
        "  - The center value (9) is greater than its neighbors (-1), which emphasizes the central pixel while subtracting the neighboring pixels.\n",
        "- **Syntax**: `cv2.filter2D(image, ddepth, kernel)`\n",
        "  - **`kernel`**: The filter kernel used to process the image.\n",
        "  - **Hyperparameters to Experiment With**:\n",
        "    - Increase the center value (`9`) to `11` or `13` for more aggressive sharpening.\n",
        "    - Reduce the negative values (`-1`) to `-2` to emphasize the sharpening effect further.\n",
        "    - Adjust kernel size to include more neighbors to see how it affects the overall image.\n",
        "  - **Why These Values**: Changing the kernel's central and surrounding values directly affects the sharpness and contrast of the image.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a68c12d",
      "metadata": {
        "id": "7a68c12d"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Image Sharpening\n",
        "kernel_sharpening = np.array([[-1, -1, -1], [-1, 9, -1], [-1, -1, -1]])\n",
        "sharpened = cv2.filter2D(image, -1, kernel_sharpening)\n",
        "display_before_after(image_rgb, cv2.cvtColor(sharpened, cv2.COLOR_BGR2RGB), 'Original Image', 'Sharpened Image')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9dbbd344",
      "metadata": {
        "id": "9dbbd344"
      },
      "source": [
        "\n",
        "## 3. Blur Image\n",
        "\n",
        "Blurring helps reduce noise and detail in an image, making it appear smoother. We use a **GaussianBlur** function.\n",
        "\n",
        "- **Syntax**: `cv2.GaussianBlur(image, (kernel_size_x, kernel_size_y), sigmaX)`\n",
        "  - **`kernel_size`**: The size of the filter kernel, given as `(width, height)`. Should be odd numbers (e.g., `(15, 15)`).\n",
        "  - **`sigmaX`**: Standard deviation in the X direction.\n",
        "  - **Hyperparameters to Experiment With**:\n",
        "    - **`kernel_size = (5, 5)`**: Smaller kernel for a minor blur effect.\n",
        "    - **`kernel_size = (25, 25)`**: Larger kernel for a strong blur effect.\n",
        "    - **`sigmaX = 0`**: Allows OpenCV to calculate the best value. Try setting `sigmaX = 5` or `10` to see how it affects the blur.\n",
        "  - **Why These Values**: The size of the kernel and the sigma value affect how much the image is blurred and how smooth the result is.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ae9b36f",
      "metadata": {
        "id": "0ae9b36f"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Blur Image\n",
        "blurred = cv2.GaussianBlur(image, (15, 15), 0)\n",
        "display_before_after(image_rgb, cv2.cvtColor(blurred, cv2.COLOR_BGR2RGB), 'Original Image', 'Blurred Image')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "149ad28d",
      "metadata": {
        "id": "149ad28d"
      },
      "source": [
        "\n",
        "## 4. Image Resize\n",
        "\n",
        "Resizing allows us to change the dimensions of an image, either scaling it up or down.\n",
        "\n",
        "- **Syntax**: `cv2.resize(image, (width, height))`\n",
        "  - **`width` and `height`**: The new dimensions of the resized image.\n",
        "  - **Hyperparameters to Experiment With**:\n",
        "    - **`width = original_width * 2`, `height = original_height * 2`**: Scaling up by a factor of 2.\n",
        "    - **`width = original_width // 2`, `height = original_height // 2`**: Scaling down by a factor of 2.\n",
        "    - **`interpolation = cv2.INTER_LINEAR`**: Use different interpolation methods such as `cv2.INTER_CUBIC` for better quality.\n",
        "  - **Why These Values**: Changing interpolation methods can affect the quality of resizing, particularly for scaling up.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20839c97",
      "metadata": {
        "id": "20839c97"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Image Resize\n",
        "resized = cv2.resize(image, (image.shape[1]//2, image.shape[0]//2))\n",
        "display_before_after(image_rgb, cv2.cvtColor(resized, cv2.COLOR_BGR2RGB), 'Original Image', 'Resized Image')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e1541d8",
      "metadata": {
        "id": "4e1541d8"
      },
      "source": [
        "\n",
        "## 5. Image Rotation\n",
        "\n",
        "Rotation changes the orientation of an image. We use a **rotation matrix** to rotate the image.\n",
        "\n",
        "- **Syntax**: `cv2.getRotationMatrix2D(center, angle, scale)`\n",
        "  - **`center`**: The center point around which the image will be rotated.\n",
        "  - **`angle`**: The angle of rotation in degrees. Positive values rotate counterclockwise.\n",
        "  - **`scale`**: Scaling factor.\n",
        "  - **Hyperparameters to Experiment With**:\n",
        "    - **`angle = 90`**: Rotate the image by 90 degrees.\n",
        "    - **`angle = 180`**: Rotate the image by 180 degrees.\n",
        "    - **`scale = 0.5`**: Reduce the size of the image during rotation.\n",
        "  - **Why These Values**: Different rotation angles and scales can be used to understand how rotating around the center affects the orientation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87de1387",
      "metadata": {
        "id": "87de1387"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Image Rotation\n",
        "(h, w) = image.shape[:2]\n",
        "center = (w // 2, h // 2)\n",
        "M = cv2.getRotationMatrix2D(center, 45, 1.0)\n",
        "rotated = cv2.warpAffine(image, M, (w, h))\n",
        "display_before_after(image_rgb, cv2.cvtColor(rotated, cv2.COLOR_BGR2RGB), 'Original Image', 'Rotated Image')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9a3d671",
      "metadata": {
        "id": "a9a3d671"
      },
      "source": [
        "\n",
        "## 9. Face Detection\n",
        "\n",
        "Face detection involves identifying faces in an image using a pre-trained **Haar Cascade classifier**.\n",
        "\n",
        "- **Syntax**: `face_cascade.detectMultiScale(image, scaleFactor, minNeighbors)`\n",
        "  - **`scaleFactor`**: Parameter specifying how much the image size is reduced at each image scale.\n",
        "  - **`minNeighbors`**: Number of neighbors each rectangle should have to retain it.\n",
        "  - **Hyperparameters to Experiment With**:\n",
        "    - **`scaleFactor = 1.05`**: Smaller scaling to find more faces, including smaller ones.\n",
        "    - **`scaleFactor = 1.2`**: Larger scaling to detect only prominent faces.\n",
        "    - **`minNeighbors = 3` or `10`**: Lower values detect more faces but may include false positives.\n",
        "  - **Why These Values**: Adjusting these hyperparameters helps balance between detecting many faces and reducing false positives.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99ff674c",
      "metadata": {
        "id": "99ff674c"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Face Detection\n",
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "faces = face_cascade.detectMultiScale(gray_image, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
        "face_detected_image = image_rgb.copy()\n",
        "for (x, y, w, h) in faces:\n",
        "    cv2.rectangle(face_detected_image, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
        "display_before_after(image_rgb, face_detected_image, 'Original Image', 'Faces Detected')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}